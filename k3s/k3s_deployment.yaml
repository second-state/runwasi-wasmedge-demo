apiVersion: apps/v1
kind: Deployment
metadata:
  name: llama-api-server
spec:
  replicas: 1
  selector:
    matchLabels:
      app: llama-api-server
  template:
    metadata:
      labels:
        app: llama-api-server
    spec:
      runtimeClassName: wasmedge
      containers:
        - name: llama-api-server
          image: ghcr.io/second-state/llama-api-server:latest
          imagePullPolicy: Never
          command: ["llama-api-server.wasm"]
          args:
            - "--prompt-template"
            - "llama-3-chat"
            - "--ctx-size"
            - "4096"
            - "--model-name"
            - "llama-3-1b"
          env:
            - name: WASMEDGE_PLUGIN_PATH
              value: "/home/runner/.wasmedge/plugin"
            - name: LD_LIBRARY_PATH
              value: "/home/runner/.wasmedge/lib"
            - name: WASMEDGE_WASINN_PRELOAD
              value: "default:GGML:CPU:/home/runner/models/Llama-3.2-1B-Instruct-Q5_K_M.gguf"
          volumeMounts:
            - name: gguf-model-file
              mountPath: /home/runner/models/Llama-3.2-1B-Instruct-Q5_K_M.gguf
              readOnly: true
            - name: wasi-nn-plugin-file
              mountPath: /home/runner/.wasmedge/plugin/libwasmedgePluginWasiNN.so
              readOnly: true
            - name: wasi-nn-plugin-lib
              mountPath: /home/runner/.wasmedge/lib
              readOnly: true
            - name: libm
              mountPath: /lib/x86_64-linux-gnu/libm.so.6
              readOnly: true
            - name: libpthread
              mountPath: /lib/x86_64-linux-gnu/libpthread.so.0
              readOnly: true
            - name: libc
              mountPath: /lib/x86_64-linux-gnu/libc.so.6
              readOnly: true
            - name: ld-linux
              mountPath: /lib64/ld-linux-x86-64.so.2
              readOnly: true
            - name: libdl
              mountPath: /lib/x86_64-linux-gnu/libdl.so.2
              readOnly: true
            - name: libstdcxx
              mountPath: /lib/x86_64-linux-gnu/libstdc++.so.6
              readOnly: true
            - name: libgcc-s
              mountPath: /lib/x86_64-linux-gnu/libgcc_s.so.1
              readOnly: true
      volumes:
        - name: gguf-model-file
          hostPath:
            path: /home/runner/models/Llama-3.2-1B-Instruct-Q5_K_M.gguf
            type: File
        - name: wasi-nn-plugin-file
          hostPath:
            path: /home/runner/.wasmedge/plugin/libwasmedgePluginWasiNN.so
            type: File
        - name: wasi-nn-plugin-lib
          hostPath:
            path: /home/runner/.wasmedge/lib
            type: Directory
        - name: libm
          hostPath:
            path: /lib/x86_64-linux-gnu/libm.so.6
            type: File
        - name: libpthread
          hostPath:
            path: /lib/x86_64-linux-gnu/libpthread.so.0
            type: File
        - name: libc
          hostPath:
            path: /lib/x86_64-linux-gnu/libc.so.6
            type: File
        - name: ld-linux
          hostPath:
            path: /lib64/ld-linux-x86-64.so.2
            type: File
        - name: libdl
          hostPath:
            path: /lib/x86_64-linux-gnu/libdl.so.2
            type: File
        - name: libstdcxx
          hostPath:
            path: /lib/x86_64-linux-gnu/libstdc++.so.6
            type: File
        - name: libgcc-s
          hostPath:
            path: /lib/x86_64-linux-gnu/libgcc_s.so.1
            type: File

---
apiVersion: v1
kind: Service
metadata:
  name: llama-api-server-service
spec:
  selector:
    app: llama-api-server
  ports:
    - protocol: TCP
      port: 8080
      targetPort: 8080
  type: ClusterIP

---
apiVersion: node.k8s.io/v1
kind: RuntimeClass
metadata:
  name: wasmedge
handler: wasmedge